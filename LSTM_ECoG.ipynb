{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a267892",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, requests\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import random\n",
    "import scipy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, SimpleRNN, GRU, Activation, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "494bb614",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23938fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @markdown Executing `set_seed(seed=seed)` you are setting the seed\n",
    "\n",
    "# For DL its critical to set the random seed so that students can have a\n",
    "# baseline to compare their results to expected results.\n",
    "# Read more here: https://pytorch.org/docs/stable/notes/randomness.html\n",
    "\n",
    "# Call `set_seed` function in the exercises to ensure reproducibility.\n",
    "\n",
    "\n",
    "def set_seed(seed=None, seed_torch=True):\n",
    "  \"\"\"\n",
    "  Function that controls randomness. NumPy and random modules must be imported.\n",
    "\n",
    "  Args:\n",
    "    seed : Integer\n",
    "      A non-negative integer that defines the random state. Default is `None`.\n",
    "    seed_torch : Boolean\n",
    "      If `True` sets the random seed for pytorch tensors, so pytorch module\n",
    "      must be imported. Default is `True`.\n",
    "\n",
    "  Returns:\n",
    "    Nothing.\n",
    "  \"\"\"\n",
    "  if seed is None:\n",
    "    seed = np.random.choice(2 ** 32)\n",
    "  random.seed(seed)\n",
    "  np.random.seed(seed)\n",
    "  if seed_torch:\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "  print(f'Random seed {seed} has been set.')\n",
    "\n",
    "\n",
    "# In case that `DataLoader` is used\n",
    "def seed_worker(worker_id):\n",
    "  \"\"\"\n",
    "  DataLoader will reseed workers following randomness in\n",
    "  multi-process data loading algorithm.\n",
    "\n",
    "  Args:\n",
    "    worker_id: integer\n",
    "      ID of subprocess to seed. 0 means that\n",
    "      the data will be loaded in the main process\n",
    "      Refer: https://pytorch.org/docs/stable/data.html#data-loading-randomness for more details\n",
    "\n",
    "  Returns:\n",
    "    Nothing\n",
    "  \"\"\"\n",
    "  worker_seed = torch.initial_seed() % 2**32\n",
    "  np.random.seed(worker_seed)\n",
    "  random.seed(worker_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b7a605d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed 145 has been set.\n"
     ]
    }
   ],
   "source": [
    "SEED = 145\n",
    "set_seed(seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c1e8e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Data retrieval\n",
    "fname = 'joystick_track.npz'\n",
    "url = \"https://osf.io/6jncm/download\"\n",
    "\n",
    "if not os.path.isfile(fname):\n",
    "  try:\n",
    "    r = requests.get(url)\n",
    "  except requests.ConnectionError:\n",
    "    print(\"!!! Failed to download data !!!\")\n",
    "  else:\n",
    "    if r.status_code != requests.codes.ok:\n",
    "      print(\"!!! Failed to download data !!!\")\n",
    "    else:\n",
    "      with open(fname, \"wb\") as fid:\n",
    "        fid.write(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8b6a401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['V', 'targetX', 'targetY', 'cursorX', 'cursorY', 'locs', 'hemisphere', 'lobe', 'gyrus', 'Brodmann_Area'])\n"
     ]
    }
   ],
   "source": [
    "# @title Data loading\n",
    "\n",
    "alldat = np.load(fname, allow_pickle=True)['dat'] # Total: 4 patients \n",
    "\n",
    "# Select just one of the recordings here. This is block 1, subject 3.\n",
    "dat = alldat[0][2]\n",
    "\n",
    "print(dat.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44efc59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_brain_regions(data, selected_BA):\n",
    "    '''\n",
    "    args: \n",
    "        data(dict): dict_keys(['V', 'targetX', 'targetY', 'cursorX', 'cursorY',\n",
    "                                'locs', 'hemisphere', 'lobe', 'gyrus', 'Brodmann_Area'])\n",
    "        selected_BA(list): a list of Brodmann Area numbers\n",
    "    retuns:\n",
    "        new_data(dict): dict_keys(['V', 'cursorX', 'cursorY'])\n",
    "    '''\n",
    "    \n",
    "    Brodmann_Area = data['Brodmann_Area']\n",
    "    BA_num = [ int(x[14:]) for x in Brodmann_Area ] # get Brodmann Area number\n",
    "    is_selected = [i in selected_BA for i in BA_num]\n",
    "    new_V = data['V'][:,is_selected].astype(np.float32) # ECog voltage (time x channel)     \n",
    "    new_data = {'V': new_V, 'cursorX': data['cursorX'], 'cursorY': data['cursorY'] }\n",
    "    \n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "da7602fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data from the electrodes in somatosensory and motor areas\n",
    "\n",
    "# Brodmann area location and function\n",
    "# https://www.simplypsychology.org/brodmann-areas.html\n",
    "# Brodmann areas 1, 2 & 3 : Primary somatosensory Cortex (postcentral gyrus) \n",
    "# Brodmann area 4 : Primary Motor Cortex (precentral gyrus)\n",
    "# Brodmann area 5 : Somatosensory Association Cortex (superior parietal lobule)\n",
    "# Brodmann area 6 : Premotor Cortex and Supplementary Motor Cortex\n",
    "# Brodmann area 7 : Somatosensory Association Cortex\n",
    "# Brodmann area 8 : Frontal eye fields\n",
    "\n",
    "selected_BA = np.arange(9) # select Brodmann area 1 to 8\n",
    "dat = select_brain_regions(dat, selected_BA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "76548851",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_signal(data, freq_band, sample_rate, order = 4):\n",
    "    '''\n",
    "    args: \n",
    "        data(dict): dict_keys(['V', 'cursorX', 'cursorY'...)\n",
    "        freq_band (list): frequence band for filtering ([lowcut, highcut])\n",
    "        sample_rate (int): sample rate (in Hz)\n",
    "        order (int): 4 (default)\n",
    "    retuns:\n",
    "        new_data(dict): dict_keys(['filtered_V', 'cursorX', 'cursorY'])\n",
    "    '''\n",
    "    nyquistFreq = sample_rate*0.5 \n",
    "    # The Nyquist frequency is the highest frequency that equipment of a given sample rate can reliably measure, \n",
    "    # one-half the given sample rate.\n",
    "    low  = freq_band[0] / nyquistFreq\n",
    "    high = freq_band[1] / nyquistFreq\n",
    "\n",
    "    b, a = scipy.signal.butter(order, [low, high], btype='bandpass')\n",
    "\n",
    "    V = data['V']\n",
    "    nt, nchan = V.shape\n",
    "\n",
    "    new_V = np.zeros_like(V)\n",
    "    for i in range(nchan):\n",
    "        new_V[:,i] = scipy.signal.filtfilt(b, a, V[:, i]) \n",
    "\n",
    "    new_data = {'V': new_V, 'cursorX': data['cursorX'], 'cursorY': data['cursorY'] }    \n",
    "    \n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ab000056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter ecog\n",
    "# Beta rhythms (13-30 Hz)\n",
    "sample_rate = 1000\n",
    "freq_band = [10, 50]\n",
    "dat = filter_signal(dat, freq_band, sample_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "54a5adbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_snips(data, window_len, stride_len, sample_rate):\n",
    "    '''\n",
    "    args:\n",
    "        data(dict): dict_keys(['V', 'cursorX', 'cursorY'])\n",
    "        window_len (int): length of time window (in ms)\n",
    "        stride_len (int): length of stride (in ms)\n",
    "        sample_rate (int): sample rate (in Hz)\n",
    "    \n",
    "    returns: \n",
    "        X: neural data (a 3D matrix, # of samples x # of bin in window_len  x # of channel) \n",
    "        Y: label (a 3D matrix, # of smaples x x # of bin in window_len x # of coordinates)\n",
    "    '''\n",
    "    bin_size = 1000/sample_rate # ms\n",
    "    V = data['V'].astype(np.float32) #ECoG recording (time x channel)\n",
    "    nt, nchan = V.shape\n",
    "    num_bin_window = int(window_len/bin_size)\n",
    "    assert len(data['cursorX']) == nt\n",
    "    assert len(data['cursorX']) == len(data['cursorY'])\n",
    "    indices = [(i, i + num_bin_window) for i in list(range(0, nt - num_bin_window +1, int(stride_len/bin_size)))]\n",
    "    \n",
    "    # normalize cursor X and Y\n",
    "    normalized_cursorX = data['cursorX']/np.max(data['cursorX'])\n",
    "    normalized_cursorY = data['cursorY']/np.max(data['cursorY'])\n",
    "\n",
    "    num_samples = len(indices)\n",
    "    #Initialize X and Y matrix\n",
    "    X = np.empty([num_samples,num_bin_window,nchan]) \n",
    "    X[:] = np.NaN\n",
    "    Y = np.empty([num_samples,num_bin_window,2])\n",
    "    Y[:] = np.NaN     \n",
    "\n",
    "    for i, indx in enumerate(indices):\n",
    "        X[i,:,:] = V[indx[0]:indx[1], :]\n",
    "        Y[i,:,0] = normalized_cursorX[indx[0]:indx[1], 0]\n",
    "        Y[i,:,1] = normalized_cursorY[indx[0]:indx[1], 0]\n",
    "        \n",
    "    return X, Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ea2ee499",
   "metadata": {},
   "outputs": [],
   "source": [
    "#screen refresh rate ~ 20Hz. So stride > 50ms.\n",
    "sample_rate = 1000 # (in Hz)\n",
    "window_len = 500\n",
    "stride_len = 100\n",
    "num_overlapping_bin = int(window_len/stride_len)\n",
    "X, Y = create_snips(dat, window_len, stride_len, sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3349610c",
   "metadata": {},
   "source": [
    "**Splitting the data into train, test and validation sets - 80% train, 10% test and 10% validation. Then, doing a 10 fold Cross Validation...so in the first fold, for example, 0-10% is validation, 10-20% is testing, and 20-100% is training.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "09c6f033",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_range_all=[[0,.1],[.1,.2],[.2,.3],[.3,.4],[.4,.5],\n",
    "                     [.5,.6],[.6,.7],[.7,.8],[.8,.9],[.9,1]]\n",
    "testing_range_all=[[.1,.2],[.2,.3],[.3,.4],[.4,.5],[.5,.6],\n",
    "                     [.6,.7],[.7,.8],[.8,.9],[.9,1],[0,.1]]\n",
    "    #Note that the training set is not aways contiguous. For example, in the second fold, the training set has 0-10% and 30-100%.\n",
    "    #In that example, we enter of list of lists: [[0,.1],[.3,1]]\n",
    "training_range_all=[[[.2,1]],[[0,.1],[.3,1]],[[0,.2],[.4,1]],[[0,.3],[.5,1]],[[0,.4],[.6,1]],\n",
    "                       [[0,.5],[.7,1]],[[0,.6],[.8,1]],[[0,.7],[.9,1]],[[0,.8]],[[.1,.9]]]\n",
    "\n",
    "num_folds=len(valid_range_all) #Number of cross validation folds\n",
    "\n",
    "    #R2 values\n",
    "mean_r2_lstm=np.empty(num_folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "209b399f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1339\n"
     ]
    }
   ],
   "source": [
    "#Actual data\n",
    "y_test_all=[]\n",
    "y_train_all=[]\n",
    "y_valid_all=[]\n",
    "\n",
    "#Test predictions\n",
    "y_pred_lstm_all=[]\n",
    "dec_error_all = []\n",
    "\n",
    "#Training predictions\n",
    "y_train_pred_lstm_all=[]\n",
    "\n",
    "#Validation predictions\n",
    "y_valid_pred_lstm_all=[]\n",
    "\n",
    "t1 = time.time()  # If I want to keep track of how much time has elapsed\n",
    "\n",
    "num_examples = len(X)  # number of examples \n",
    "print(num_examples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a2a6268a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMRegression(object):\n",
    "\n",
    "    \"\"\"\n",
    "    Class for the LSTM decoder for  Regression\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    units: integer, optional, default 400\n",
    "        Number of hidden units in each layer\n",
    "\n",
    "    dropout: decimal, optional, default 0.2\n",
    "        Proportion of units that get dropped out\n",
    "\n",
    "    num_epochs: integer, optional, default 10\n",
    "        Number of epochs used for training\n",
    "\n",
    "    verbose: binary, optional, default=0\n",
    "        Whether to show progress of the fit after each epoch\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,units=400,dropout=0.2,num_epochs=10,verbose=0):\n",
    "         self.units=units\n",
    "         self.dropout=dropout\n",
    "         self.num_epochs=num_epochs\n",
    "         self.verbose=verbose\n",
    "\n",
    "\n",
    "    def fit(self,X_train,y_train):\n",
    "\n",
    "        \"\"\"\n",
    "        Train LSTM Decoder\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X_train: numpy 3d array of shape [n_samples,n_time_bins,n_channels]\n",
    "            This is the neural data.\n",
    "            See example file for an example of how to format the neural data correctly\n",
    "\n",
    "        y_train: numpy 2d array of shape [n_samples, n_outputs]\n",
    "            This is the outputs that are being predicted\n",
    "        \"\"\"\n",
    "\n",
    "        model=Sequential() #Declare model\n",
    "        #Add recurrent layer\n",
    "        model.add(LSTM(self.units,input_shape=(X_train.shape[1],X_train.shape[2]),dropout=self.dropout,recurrent_dropout=self.dropout)) #Within recurrent layer, include dropout\n",
    "        if self.dropout!=0: \n",
    "            model.add(Dropout(self.dropout)) #Dropout some units (recurrent layer output units)\n",
    "\n",
    "        #Add dense connections to output layer\n",
    "        model.add(Dense(y_train.shape[1]))\n",
    "\n",
    "        #Fit model (and set fitting parameters)\n",
    "        model.compile(loss='mse',optimizer='rmsprop',metrics=['accuracy']) #Set loss function and optimizer\n",
    "        model.fit(X_train,y_train,epochs=self.num_epochs,verbose=self.verbose) #Fit the model\n",
    "        self.model=model\n",
    "\n",
    "\n",
    "    def predict(self,X_test):\n",
    "\n",
    "        \"\"\"\n",
    "        Predict outcomes using trained LSTM Decoder\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X_test: numpy 3d array of shape [n_samples,n_time_bins,n_neurons]\n",
    "            This is the neural data being used to predict outputs.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y_test_predicted: numpy 2d array of shape [n_samples,n_outputs]\n",
    "            The predicted outputs\n",
    "        \"\"\"\n",
    "\n",
    "        y_test_predicted = self.model.predict(X_test) #Make predictions\n",
    "        return y_test_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82fde78",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get hyperparameters using Bayesian optimization based on validation set R2 values###\n",
    "\n",
    "# Define a function that returns the metric we are trying to optimize (R2 value of the validation set)\n",
    "# as a function of the hyperparameter we are fitting\n",
    "def lstm_evaluate(num_units, frac_dropout, n_epochs):\n",
    "    num_units = int(num_units)\n",
    "    frac_dropout = float(frac_dropout)\n",
    "    n_epochs = int(n_epochs)\n",
    "    model_lstm = LSTMRegression(units=num_units, dropout=frac_dropout, num_epochs=n_epochs)\n",
    "    model_lstm.fit(X_train, y_train)\n",
    "    y_valid_predicted_lstm = model_lstm.predict(X_valid)\n",
    "    return np.mean(get_R2(y_valid, y_valid_predicted_lstm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc71bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do bayesian optimization\n",
    "lstmBO = BayesianOptimization(lstm_evaluate,\n",
    "                              {'num_units': (50, 600), 'frac_dropout': (0, .5), 'n_epochs': (2, 21)})\n",
    "lstmBO.maximize(init_points=20, n_iter=20, kappa=10)\n",
    "best_params = lstmBO.max['params']\n",
    "frac_dropout = float(best_params['frac_dropout'])\n",
    "n_epochs = np.int(best_params['n_epochs'])\n",
    "num_units = np.int(best_params['num_units'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c537ae80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_R2(y_test,y_test_pred):\n",
    "\n",
    "    \"\"\"\n",
    "    Function to get R2\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_test - the true outputs (a matrix of size number of examples x number of outputs)\n",
    "    y_test_pred - the predicted outputs (a matrix of size number of examples x number of outputs)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    R2_array: An array of R2s for each output\n",
    "    \"\"\"\n",
    "\n",
    "    R2_list=[] #Initialize a list that will contain the R2s for all the outputs\n",
    "    for i in range(y_test.shape[1]): #Loop through outputs\n",
    "        #Compute R2 for each output\n",
    "        y_mean=np.mean(y_test[:,i])\n",
    "        R2=1-np.sum((y_test_pred[:,i]-y_test[:,i])**2)/np.sum((y_test[:,i]-y_mean)**2)\n",
    "        R2_list.append(R2) #Append R2 of this output to the list\n",
    "    R2_array=np.array(R2_list)\n",
    "    return R2_array #Return an array of R2s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d58a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_folder= \"/Users/yiting/Documents/NMA_DL/Project/Movement_BCI/LSTM\"\n",
    "cursor_axis = 0 # 0 for x coordinate, 1 for y coordinate\n",
    "\n",
    "for i in range(num_folds):  # Loop through the folds\n",
    "\n",
    "    ######### SPLIT DATA INTO TRAINING/TESTING/VALIDATION #########\n",
    "\n",
    "    # Get testing set for this fold\n",
    "    testing_range = testing_range_all[i]\n",
    "    testing_set = np.arange(int(np.round(testing_range[0] * num_examples)) + num_overlapping_bin,\n",
    "                            int(np.round(testing_range[1] * num_examples)))\n",
    "\n",
    "    # Get validation set for this fold\n",
    "    valid_range = valid_range_all[i]\n",
    "    valid_set = np.arange(int(np.round(valid_range[0] * num_examples)) + num_overlapping_bin,\n",
    "                            int(np.round(valid_range[1] * num_examples)))\n",
    "\n",
    "    # Get training set for this fold.\n",
    "    # Note this needs to take into account a non-contiguous training set (see section 3C)\n",
    "    training_ranges = training_range_all[i]\n",
    "    for j in range(len(training_ranges)):  # Go through different separated portions of the training set\n",
    "        training_range = training_ranges[j]\n",
    "        if j == 0:  # If it's the first portion of the training set, make it the training set\n",
    "            training_set = np.arange(int(np.round(training_range[0] * num_examples)) + num_overlapping_bin,\n",
    "                                        int(np.round(training_range[1] * num_examples)))\n",
    "        if j == 1:  # If it's the second portion of the training set, concatentate it to the first\n",
    "            training_set_temp = np.arange(int(np.round(training_range[0] * num_examples)) + num_overlapping_bin,\n",
    "                                            int(np.round(training_range[1] * num_examples)))\n",
    "            training_set = np.concatenate((training_set, training_set_temp), axis=0)\n",
    "\n",
    "    # Get training data\n",
    "    X_train = X[training_set, :]\n",
    "    y_train = Y[training_set, :, cursor_axis]\n",
    "\n",
    "    # Get testing data\n",
    "    X_test = X[testing_set, :]\n",
    "    y_test = Y[testing_set, :, cursor_axis]\n",
    "\n",
    "    # Get validation data\n",
    "    X_valid = X[valid_set, :]\n",
    "    y_valid = Y[valid_set, :, cursor_axis]\n",
    "\n",
    "\n",
    "    # Add actual train/valid/test data to lists (for saving)\n",
    "    y_test_all.append(y_test)\n",
    "    y_train_all.append(y_train)\n",
    "    y_valid_all.append(y_valid)\n",
    "\n",
    "    # Fit LSTM\n",
    "    # Run model w/ below hyperparameters\n",
    "    model_lstm = LSTMRegression(units=200, dropout=0.3, num_epochs=10)\n",
    "    model_lstm.fit(X_train, y_train)\n",
    "    y_test_predicted_lstm = model_lstm.predict(X_test)\n",
    "    mean_r2_lstm[i] = np.mean(get_R2(y_test, y_test_predicted_lstm))\n",
    "    # Print test set R2\n",
    "    R2s_lstm = get_R2(y_test, y_test_predicted_lstm)\n",
    "    print('R2s:', R2s_lstm)\n",
    "    # Add predictions of training/validation/testing to lists (for saving)\n",
    "    y_pred_lstm_all.append(y_test_predicted_lstm)\n",
    "    y_train_pred_lstm_all.append(model_lstm.predict(X_train))\n",
    "    y_valid_pred_lstm_all.append(model_lstm.predict(X_valid))\n",
    "\n",
    "save_obj = {'y_test_all':y_test_all, 'y_valid_all': y_valid_all, 'y_train_all': y_train_all, 'mean_r2_lstm': mean_r2_lstm, 'y_pred_lstm_all':y_pred_lstm_all, 'y_train_pred_lstm_all':y_train_pred_lstm_all, 'y_valid_pred_lstm_all':y_valid_pred_lstm_all, 'X': X, 'Y': Y}\n",
    "\n",
    "with open(save_folder + 'results_lstm_regression.pickle', 'wb') as f:\n",
    "    pickle.dump(save_obj, f) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93da6581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with previous RNN \n",
    "# If the difference between the true and predicted cursor position is less than 0.1, it is considered as correct. \n",
    "# The performance of previous RNN model is 54%\n",
    "diff = [np.linalg.norm(predicted_labels[i] - actual_labels[i]) for i in range(len(predicted_labels))]\n",
    "precentages = len(np.where(np.array(diff) < 0.1)[0])/len(diff)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
